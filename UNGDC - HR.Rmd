---
title: "UNGDC"
author: "JGUP"
date: "10/10/2019"
output: html_document
---

```{r setup, include=FALSE}
library(zip)


```

1
### PTS and UNGDC data

```{r}
resource <- paste0(getwd(),'/res')
if(!dir.exists(resource)){
dir.create(resource)

download.file('https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/0TJX8Y/PZUURT', destfile=(paste0(temp<- tempfile())),mode="wb")
unzip(zipfile=paste0(temp), exdir = resource)
}
root_folder <- paste0(resource,'/Converted sessions/')

filelist <- list.files(path = root_folder, pattern = '*.txt', recursive = TRUE)
filenames <- basename(filelist)
ungdc_files <- data.frame(t(sapply(strsplit(filenames, '_'), 
                 function(x) c(x[1], x[2], substr(x[3], 1, 4)))))
colnames(ungdc_files) <- c('country', 'number', 'year')

ungdc_files$text <- sapply(paste0(root_folder, filelist), function(x) readChar(x, file.info(x)$size))
load(url("http://www.politicalterrorscale.org/Data/Files/PTS-2019.RData"))

unlink(temp)
rm(temp)
rm(filelist)
rm(filenames)
rm(root_folder)
```

### add LSD Lexicoder Sentiment Dictionary: we have decided not to use lexicoder but to use the Syuzhet dictionary instead (so would want to delete this)

install.packages("quanteda")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("kbenoit/quanteda.dictionaries")
install.packages("readtext")
install.packages("devtools")

install.packages("newsmap")
install.packages("tm")
install.packages("NLP")
install.packages("tibble")
install.packages("slam")

library(quanteda)
library(readtext)
library(quanteda.dictionaries)
#?quanteda.dictionaries
data(data_dictionary_LSD2015)
str(data_dictionary_LSD2015)
pos.LSD <- data_dictionary_LSD2015[['positive']]
neg.LSD <- data_dictionary_LSD2015[['negative']]
#differentiating positive nad negative words within LSD2015
#there is also pos.negative and neg.positive categories
sample(pos.LSD, 10)
sample(neg.LSD, 10)


### add Syuzhet dictionary

```{r}
install.packages("syuzhet")
install.packages("devtools")
install.packages("pander")
devtools::install_github("mjockers/syuzhet")
1
library(syuzhet)
library(pander)

#load the text as tokens to be analysed for sentiment

?get_sentiment
mode(ungdc_files)
str(ungdc_files$text)
toks_UNEX <- get_tokens(ungdc_files$text[1], pattern = "\\W")

#get sentiment analysis with four different dictionaries: syuzhet (1), afinn, bing, and nrc
#1: Create vectors for the sentiment dictionaries: syuzhet is mixed, bing is binary neg/pos, afinn is a scale and nrc has different emptional categories

sent_syuzhet <- get_sentiment(toks_UNEX, method ="syuzhet")
sent_bing <- get_sentiment(toks_UNEX, method ="afinn")
sent_afinn <- get_sentiment(toks_UNEX, method ="bing")
sent_nrc <- get_sentiment(toks_UNEX, method ="nrc", lang = "english")

#2: get values

head(sent_syuzhet, n = 100)
head(sent_bing, n = 100)
head(sent_afinn, n = 100)
head(sent_nrc, n = 100)

#3: descriptive analysis

mean(sent_syuzhet)
summary(sent_syuzhet)
mean(sent_afinn)
summary(sent_afinn)

plot(
  sent_syuzhet, 
  type="h", 
  main="Example Plot of Speech", 
  xlab = "Speech Time", 
  ylab= "Emotional Valence"
  )

#4: nrc dictionary

cat_nrc <- get_nrc_sentiment(toks_UNEX)
angry_toks <- which(cat_nrc$anger>0)
toks_UNEX[angry_toks]
#table of categories in emotions
pander::pandoc.table(cat_nrc[1:10, 1:8], split.table = Inf)
#table of negative vs positive
pander::pandoc.table(cat_nrc[1:10, 9:10], split.table = Inf)

#graph of categories in emotions in percentages - nice! this graph shows what kind of emotions they use in the first speech
?barplot
barplot(
  sort(colSums(prop.table(cat_nrc[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1, 
  main = "Emotions in UN example", xlab="Percentage"
  )

```

### add international military actors dictionary

```{r}
download.file()
urlint <- 'https://raw.githubusercontent.com/openeventdata/tabari_dictionaries/master/IntMilitGroups.090831.actors.txt'
intmilact <- readtext(urlint)

```

# import the Laver-Garry dictionary from Provalis Research
dictfile <- tempfile()
download.file("https://provalisresearch.com/Download/LaverGarry.zip", 
              dictfile, mode = "wb")
unzip(dictfile, exdir = (td <- tempdir()))
dictlg <- dictionary(file = paste(td, "LaverGarry.cat", sep = "/"))
head(dfm(data_corpus_inaugural, dictionary = dictlg))

# import a LIWC formatted dictionary from http://www.moralfoundations.org
download.file("https://goo.gl/5gmwXq", tf <- tempfile())
dictliwc <- dictionary(file = tf, format = "LIWC")
head(dfm(data_corpus_inaugural, dictionary = dictliwc))

According to Greene and Park, we could add the following words to the human rights dictionary (based on PTS, US State Human Rights Report, and Richard (2005):

period 1978-1989:
abducted
abuses
africa
air
areas
armed
army
assassination
assassinations
assistance
attacks
brutal
busnesses
captured
casualties
caused
civilians
command
common
controlled
destroyed
disappearances
disappeared
displaced
el
executed
execution
executions
exit
extensive
fighting
fled
forced
forces
frequently
government
groups
guerrilla
guerrillas
hundreds
icrc
innocent
insurgency
insurgent
insurgents
interpretation
investigate
iran

window 1990-2005
abuses
according
action
aircraft
areas
armed
army
arrested
aspect
bandits
bonded
captured
cease
civilian
civilians
collaborating
commanders
committed
common
congo
conscripted
continued
controlled
displaced
displacement
dominated
example
executed
executions
extensive
extortion
factions
fighting
fled
food
force
forced
forces
frequently
genocide
government
groups
guerrillas
human
humanitarian
hundreds
icrc
including

summary:
according
reported interpretation
reliable
committed
scale
victims
populations
armed
army
commanders
conscripted
assassinations
regime
massacres
civilians
internally
displaced
food
humanitarian
landmines
money
raped
disappearances
frequently
numerous
widespread
common
regularly
routinely
systematic extensive
